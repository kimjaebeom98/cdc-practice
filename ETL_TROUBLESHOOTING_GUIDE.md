# ETL íŒŒì´í”„ë¼ì¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ğŸ“‹ ë¬¸ì œ í•´ê²° ê³¼ì • ìš”ì•½

### ì´ˆê¸° ìƒí™©

- âœ… CDC íŒŒì´í”„ë¼ì¸ (MySQL â†’ Debezium â†’ Kafka)ì€ ì •ìƒ ì‘ë™
- âŒ ETL íŒŒì´í”„ë¼ì¸ (Kafka â†’ PostgreSQL)ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ
- âŒ Consumerê°€ ë©”ì‹œì§€ë¥¼ ì½ì§€ ëª»í•˜ê±°ë‚˜ DELETE ë©”ì‹œì§€ë§Œ ë°˜ë³µ ì²˜ë¦¬

---

## ğŸ” ë¬¸ì œ ì§„ë‹¨ ê³¼ì •

### 1ë‹¨ê³„: Kafka ë©”ì‹œì§€ ì¡´ì¬ í™•ì¸

```bash
# Kafka í† í”½ì˜ ë©”ì‹œì§€ ê°œìˆ˜ í™•ì¸
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell --bootstrap-server localhost:9092 --topic dbserver1.bankdb.bank_accounts
```

**ê²°ê³¼**: `dbserver1.bankdb.bank_accounts:0:14` - 14ê°œ ë©”ì‹œì§€ ì¡´ì¬ í™•ì¸

### 2ë‹¨ê³„: ê°„ë‹¨í•œ Consumer í…ŒìŠ¤íŠ¸

```python
# test_kafka_consumer.py ìƒì„± ë° í…ŒìŠ¤íŠ¸
consumer = KafkaConsumer(
    'dbserver1.bankdb.bank_accounts',
    bootstrap_servers=['kafka:9092'],  # ì¤‘ìš”: localhostê°€ ì•„ë‹Œ kafka:9092
    group_id='test-consumer-group',
    auto_offset_reset='earliest',
    consumer_timeout_ms=10000,
    value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None
)
```

**ê²°ê³¼**: âœ… ConsumerëŠ” ì •ìƒ ì‘ë™, INSERT ë©”ì‹œì§€ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ ì½ìŒ

### 3ë‹¨ê³„: Airflow DAG ë¬¸ì œ ì§„ë‹¨

```bash
# Airflow DAG ì‹¤í–‰ ë¡œê·¸ í™•ì¸
docker exec airflow-scheduler airflow tasks test kafka_to_postgres_etl consume_kafka_messages 2025-09-18
```

**ë°œê²¬ëœ ë¬¸ì œë“¤**:

1. **íŒŒí‹°ì…˜ í• ë‹¹ ë¬¸ì œ**: `Updated partition assignment: []` - Consumerê°€ íŒŒí‹°ì…˜ì— í• ë‹¹ë˜ì§€ ì•ŠìŒ
2. **Consumer Group ì˜¤í”„ì…‹ ë¬¸ì œ**: Consumerê°€ ë©”ì‹œì§€ë¥¼ ì½ì—ˆì§€ë§Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
3. **DELETE ë©”ì‹œì§€ ë°˜ë³µ ì²˜ë¦¬**: ê°™ì€ ì˜¤í”„ì…‹ì˜ DELETE ë©”ì‹œì§€ë§Œ ê³„ì† ì²˜ë¦¬

**ì‹¤ì œ ì˜¤ë¥˜ ë¡œê·¸**:

```
[2025-09-18T11:32:00.459+0000] {subscription_state.py:257} INFO - Updated partition assignment: []
[2025-09-18T11:32:19.729+0000] {kafka_to_postgres_etl.py:130} INFO - ğŸ‰ ì´ 0ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ
```

**Consumer Group ìƒíƒœ í™•ì¸**:

```bash
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 --group airflow-etl-simple --describe
```

**ê²°ê³¼**: `CURRENT-OFFSET=13, LOG-END-OFFSET=14, LAG=1` - ë©”ì‹œì§€ë¥¼ ì½ì—ˆì§€ë§Œ ì²˜ë¦¬í•˜ì§€ ëª»í•¨

---

## ğŸ› ï¸ í•´ê²° ë°©ë²•

### ë¬¸ì œ 1: íŒŒí‹°ì…˜ í• ë‹¹ ì‹¤íŒ¨

**ì›ì¸**: Consumer Group ì„¤ì • ë¬¸ì œë¡œ ì¸í•œ íŒŒí‹°ì…˜ í• ë‹¹ ì‹¤íŒ¨

**í•´ê²°ì±…**:

```python
# ê¸°ì¡´ ë¬¸ì œê°€ ìˆë˜ ì„¤ì •
consumer = KafkaConsumer(
    'dbserver1.bankdb.bank_accounts',
    bootstrap_servers=['kafka:9092'],
    group_id='airflow-etl-simple',  # Consumer Group ì‚¬ìš©
    auto_offset_reset='earliest',
    consumer_timeout_ms=20000,
    enable_auto_commit=False,
    value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None
)

# í•´ê²°ëœ ì„¤ì •
consumer = KafkaConsumer(
    'dbserver1.bankdb.bank_accounts',
    bootstrap_servers=['kafka:9092'],
    group_id=None,  # Consumer Group ì‚¬ìš© ì•ˆ í•¨
    auto_offset_reset='earliest',
    consumer_timeout_ms=15000,
    value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None
)
```

### ë¬¸ì œ 2: ë©”ì‹œì§€ ë°˜ë³µ ì²˜ë¦¬

**ì›ì¸**: Consumer Group ì˜¤í”„ì…‹ ê´€ë¦¬ ë¬¸ì œë¡œ ì¸í•œ ê°™ì€ ë©”ì‹œì§€ ë°˜ë³µ ì²˜ë¦¬

**ê¸°ì¡´ DAGì˜ êµ¬ì²´ì  ë¬¸ì œì **:

```python
# kafka_to_postgres_etl.pyì˜ ë¬¸ì œê°€ ìˆë˜ ë¶€ë¶„
consumer = KafkaConsumer(
    'dbserver1.bankdb.bank_accounts',
    bootstrap_servers=['kafka:9092'],
    group_id=f'airflow-etl-{int(time.time())}',  # ë™ì  ê·¸ë£¹ ID
    auto_offset_reset='earliest',
    consumer_timeout_ms=30000,
    enable_auto_commit=True,
    auto_commit_interval_ms=5000,
    session_timeout_ms=30000,
    heartbeat_interval_ms=10000,
    value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None
)
```

**ë¬¸ì œì **:

1. **ë™ì  Consumer Group ID**: ë§¤ë²ˆ ìƒˆë¡œìš´ ê·¸ë£¹ì´ ìƒì„±ë˜ì–´ ì˜¤í”„ì…‹ ê´€ë¦¬ í˜¼ë€
2. **ë³µì¡í•œ ì„¤ì •**: ê³¼ë„í•œ íƒ€ì„ì•„ì›ƒê³¼ ì»¤ë°‹ ì„¤ì •ìœ¼ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„±
3. **ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§ ë³µì¡ì„±**: ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë¡œì§ì´ ë³µì¡í•˜ì—¬ ì˜¤ë¥˜ ë°œìƒ

**í•´ê²°ì±…**: ì™„ì „íˆ ìƒˆë¡œìš´ DAG ì‘ì„± (`simple_kafka_etl.py`)

### ë¬¸ì œ 3: ë³µì¡í•œ Consumer ì„¤ì •

**ì›ì¸**: ê³¼ë„í•˜ê²Œ ë³µì¡í•œ Consumer ì„¤ì •ìœ¼ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„±

**ê¸°ì¡´ DAGì˜ ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§ ë¬¸ì œ**:

```python
# kafka_to_postgres_etl.pyì˜ ë³µì¡í•œ ì²˜ë¦¬ ë¡œì§
def process_insert_message(cursor, data, message):
    # ë³µì¡í•œ ë°ì´í„° ì¶”ì¶œ ë¡œì§
    original_id = data.get('id')
    user_id = data.get('user_id')
    account = data.get('account')
    registered_at = data.get('registered_at')

    # ì¤‘ë³µ í™•ì¸ ë¡œì§
    cursor.execute("SELECT COUNT(*) FROM bank_accounts_current WHERE original_id = %s", (original_id,))
    existing_count = cursor.fetchone()[0]

    if existing_count == 0:
        # INSERT ë¡œì§
        cursor.execute("""
            INSERT INTO bank_accounts_current
            (original_id, user_id, account, original_registered_at, last_updated_at)
            VALUES (%s, %s, %s, %s, NOW())
        """, (original_id, user_id, account, registered_at))

        # íˆìŠ¤í† ë¦¬ ê¸°ë¡
        cursor.execute("""
            INSERT INTO bank_accounts_history
            (original_id, user_id, account, change_type, change_timestamp,
             original_registered_at, kafka_offset, kafka_partition, kafka_topic)
            VALUES (%s, %s, %s, %s, NOW(), %s, %s, %s, %s)
        """, (original_id, user_id, account, 'INSERT', registered_at,
              message.offset, message.partition, message.topic))
```

**ë¬¸ì œì **:

1. **í•¨ìˆ˜ ë¶„ë¦¬ë¡œ ì¸í•œ ë³µì¡ì„±**: `process_insert_message`, `process_update_message`, `process_delete_message` í•¨ìˆ˜ë“¤ì´ ê°ê° ë³µì¡
2. **ì˜¤ë¥˜ ì²˜ë¦¬ ë¶€ì¡±**: ê° í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬ê°€ ë¶€ì¡±í•˜ì—¬ í•˜ë‚˜ì˜ ì˜¤ë¥˜ê°€ ì „ì²´ë¥¼ ì¤‘ë‹¨ì‹œí‚´
3. **íŠ¸ëœì­ì…˜ ê´€ë¦¬**: ê° ë©”ì‹œì§€ë³„ë¡œ ê°œë³„ ì»¤ë°‹ì´ ì•„ë‹Œ ë°°ì¹˜ ì»¤ë°‹ìœ¼ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„±

**ìƒˆë¡œìš´ DAGì˜ ê°„ë‹¨í•œ í•´ê²°ì±…**:

```python
# simple_kafka_etl.pyì˜ ê°„ë‹¨í•œ ì²˜ë¦¬ ë¡œì§
for message in consumer:
    try:
        data = message.value
        if not data:
            continue

        processed_count += 1
        logging.info(f"ğŸ“¨ ë©”ì‹œì§€ {processed_count}: {data}")

        # DELETE ë©”ì‹œì§€ ì²˜ë¦¬
        if data.get('__deleted') == 'true':
            # ê°„ë‹¨í•œ DELETE ë¡œì§
            cursor.execute("DELETE FROM bank_accounts_current WHERE original_id = %s", (data.get('id'),))
            # íˆìŠ¤í† ë¦¬ ê¸°ë¡
            cursor.execute("INSERT INTO bank_accounts_history ...")

        # INSERT/UPDATE ë©”ì‹œì§€ ì²˜ë¦¬
        elif data.get('__deleted') == 'false':
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            cursor.execute("SELECT COUNT(*) FROM bank_accounts_current WHERE original_id = %s", (data.get('id'),))
            existing_count = cursor.fetchone()[0]

            if existing_count > 0:
                # UPDATE ë¡œì§
                cursor.execute("UPDATE bank_accounts_current SET ...")
            else:
                # INSERT ë¡œì§
                cursor.execute("INSERT INTO bank_accounts_current ...")

        # ê° ë©”ì‹œì§€ ì²˜ë¦¬ í›„ ì¦‰ì‹œ ì»¤ë°‹
        conn.commit()
        logging.info(f"âœ… ë©”ì‹œì§€ {processed_count} ì²˜ë¦¬ ì™„ë£Œ")

    except Exception as e:
        logging.error(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        conn.rollback()
        continue
```

**í•µì‹¬ ê°œì„ ì‚¬í•­**:

1. **ë‹¨ì¼ ë£¨í”„**: ëª¨ë“  ì²˜ë¦¬ë¥¼ í•˜ë‚˜ì˜ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•˜ì—¬ ë³µì¡ì„± ì œê±°
2. **ì¦‰ì‹œ ì»¤ë°‹**: ê° ë©”ì‹œì§€ ì²˜ë¦¬ í›„ ì¦‰ì‹œ ì»¤ë°‹í•˜ì—¬ íŠ¸ëœì­ì…˜ ì•ˆì •ì„± í™•ë³´
3. **ê°„ë‹¨í•œ ì˜ˆì™¸ ì²˜ë¦¬**: try-catchë¡œ ê° ë©”ì‹œì§€ë³„ ë…ë¦½ì  ì²˜ë¦¬
4. **ëª…í™•í•œ ë¡œê¹…**: ê° ë‹¨ê³„ë³„ ìƒì„¸í•œ ë¡œê·¸ë¡œ ë””ë²„ê¹… ìš©ì´

---

## ğŸ“Š ì„±ê³µ ê²°ê³¼

### ìµœì¢… ì„±ê³µ ë¡œê·¸

```
[2025-09-18T11:35:38.731+0000] {simple_kafka_etl.py:38} INFO - ğŸš€ ê°„ë‹¨í•œ Kafka ETL ì‹œì‘
[2025-09-18T11:35:38.851+0000] {subscription_state.py:257} INFO - Updated partition assignment: [('dbserver1.bankdb.bank_accounts', 0)]
[2025-09-18T11:35:38.959+0000] {simple_kafka_etl.py:67} INFO - ğŸ“¨ ë©”ì‹œì§€ 1: {'id': 1, 'user_id': 1001, 'account': 'KB Bank 123-456-789012', 'registered_at': '2025-09-18T01:52:22Z', '__deleted': 'false'}
[2025-09-18T11:35:38.960+0000] {simple_kafka_etl.py:89} INFO - â• INSERT/UPDATE ì²˜ë¦¬: 1
[2025-09-18T11:35:38.961+0000] {simple_kafka_etl.py:116} INFO - â• INSERT: 1
[2025-09-18T11:35:38.964+0000] {simple_kafka_etl.py:138} INFO - âœ… ë©”ì‹œì§€ 1 ì²˜ë¦¬ ì™„ë£Œ
...
[2025-09-18T11:35:38.978+0000] {simple_kafka_etl.py:150} INFO - ğŸ‰ ì´ 10ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ
```

### PostgreSQL ê²°ê³¼

**í˜„ì¬ ìƒíƒœ í…Œì´ë¸”**: 7ê°œ ê³„ì¢Œ ì •ë³´ ì €ì¥
**íˆìŠ¤í† ë¦¬ í…Œì´ë¸”**: ëª¨ë“  ë³€ê²½ì‚¬í•­ (INSERT, UPDATE, DELETE) ê¸°ë¡

---

## ğŸ”§ í•µì‹¬ í•´ê²° í¬ì¸íŠ¸

### 1. Consumer Group ì œê±°

- `group_id=None`ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ íŒŒí‹°ì…˜ í• ë‹¹ ë¬¸ì œ í•´ê²°
- ì˜¤í”„ì…‹ ê´€ë¦¬ ë³µì¡ì„± ì œê±°

### 2. ê°„ë‹¨í•œ ì„¤ì • ì‚¬ìš©

- ìµœì†Œí•œì˜ Consumer ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- ë¶ˆí•„ìš”í•œ ê³ ê¸‰ ê¸°ëŠ¥ ì œê±°

### 3. ì¦‰ì‹œ ì»¤ë°‹

- ê° ë©”ì‹œì§€ ì²˜ë¦¬ í›„ ì¦‰ì‹œ PostgreSQL ì»¤ë°‹
- íŠ¸ëœì­ì…˜ ì•ˆì •ì„± í™•ë³´

### 4. ëª…í™•í•œ ë¡œê¹…

- ê° ë‹¨ê³„ë³„ ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
- ë¬¸ì œ ë°œìƒ ì‹œ ë¹ ë¥¸ ì§„ë‹¨ ê°€ëŠ¥

### 5. ì½”ë“œ êµ¬ì¡° ë‹¨ìˆœí™”

**ê¸°ì¡´ DAG (kafka_to_postgres_etl.py)**:

- ë³µì¡í•œ í•¨ìˆ˜ ë¶„ë¦¬ (`process_insert_message`, `process_update_message`, `process_delete_message`)
- ë°°ì¹˜ ì»¤ë°‹ìœ¼ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„±
- ë™ì  Consumer Group IDë¡œ ì¸í•œ ì˜¤í”„ì…‹ í˜¼ë€

**ìƒˆë¡œìš´ DAG (simple_kafka_etl.py)**:

- ë‹¨ì¼ ë£¨í”„ì—ì„œ ëª¨ë“  ì²˜ë¦¬
- ê° ë©”ì‹œì§€ë³„ ì¦‰ì‹œ ì»¤ë°‹
- Consumer Group ì—†ì´ ì§ì ‘ ë©”ì‹œì§€ ì²˜ë¦¬
- ê°„ë‹¨í•œ ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´

---

## ğŸ“ êµí›ˆ

1. **ë‹¨ìˆœí•¨ì´ ìµœê³ **: ë³µì¡í•œ ì„¤ì •ë³´ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì„¤ì •ì´ ë” ì¢‹ìŒ
2. **Consumer Group ì‹ ì¤‘ ì‚¬ìš©**: í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´ Consumer Group ì‚¬ìš©ì„ í”¼í•  ê²ƒ
3. **ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸**: ì „ì²´ íŒŒì´í”„ë¼ì¸ë³´ë‹¤ëŠ” ê° êµ¬ì„±ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
4. **ìƒì„¸í•œ ë¡œê¹…**: ë¬¸ì œ ë°œìƒ ì‹œ ë¹ ë¥¸ ì§„ë‹¨ì„ ìœ„í•œ ë¡œê¹… í•„ìˆ˜

---

## ğŸ”„ ìµœì‹  íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Airflow ìë™ ì‹¤í–‰ ë¬¸ì œ)

### ë¬¸ì œ ìƒí™©

**ì‚¬ìš©ì ì§ˆë¬¸**: "ì—¥ ê°‘ìê¸° ì™œ ë˜ëŠ”ê±°ì•¼? dagë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰ì‹œì¼œì„œ ê·¸ëŸ°ê±°ì•¼? ê·¸ëŸ¬ë©´ @test-etl-pipeline.sh ì—ì„œ ìë™ìœ¼ë¡œ dagë¥¼ ì‹¤í–‰ì‹œí‚¤ë©´ ì•ˆë˜ëŠ”ê±° ì•„ë‹ˆì•¼?"

**ë°œê²¬ëœ ë¬¸ì œ**:

- âœ… DAG ìˆ˜ë™ ì‹¤í–‰ ì‹œì—ëŠ” ì •ìƒ ì‘ë™
- âŒ `test-etl-pipeline.sh` ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìë™ ì‹¤í–‰ ì‹œ ì‹¤íŒ¨
- âŒ DAGê°€ `queued` ìƒíƒœë¡œ ë©ˆì¶°ìˆìŒ

### ë¬¸ì œ ì§„ë‹¨ ê³¼ì •

#### 1ë‹¨ê³„: DAG ìƒíƒœ í™•ì¸

```bash
# DAG ì‹¤í–‰ ìƒíƒœ í™•ì¸
docker exec airflow-webserver airflow dags list-runs -d simple_kafka_etl
```

**ê²°ê³¼**:

```
conf | dag_id           | dag_run_id           | state
=====+==================+======================+=======
{}   | simple_kafka_etl | manual__2025-09-18T1 | queued
     |                  | 2:45:25+00:00        |
```

**ë¬¸ì œ ë°œê²¬**: DAGê°€ `queued` ìƒíƒœë¡œ ë©ˆì¶°ìˆì–´ì„œ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

#### 2ë‹¨ê³„: Airflow Scheduler ìƒíƒœ í™•ì¸

```bash
# Airflow Scheduler ë¡œê·¸ í™•ì¸
docker logs airflow-scheduler --tail 20
```

**ê²°ê³¼**: SchedulerëŠ” ì •ìƒ ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ DAGë¥¼ ì‹¤í–‰í•˜ì§€ ëª»í•¨

#### 3ë‹¨ê³„: Airflow ì„¤ì • í™•ì¸

**ë°œê²¬ëœ í•µì‹¬ ë¬¸ì œ**:

```yaml
# docker-compose.ymlì˜ ë¬¸ì œê°€ ìˆë˜ ì„¤ì •
environment:
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true" # âŒ ë¬¸ì œ!
```

**ë¬¸ì œì **: DAGê°€ ìƒì„±ë  ë•Œ ìë™ìœ¼ë¡œ ì¼ì‹œì •ì§€(paused) ìƒíƒœë¡œ ì„¤ì •ë¨

### í•´ê²° ë°©ë²•

#### 1ë‹¨ê³„: Airflow ì„¤ì • ìˆ˜ì •

```yaml
# docker-compose.yml ìˆ˜ì •
environment:
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false" # âœ… í•´ê²°!
```

**ìˆ˜ì • ìœ„ì¹˜**: `airflow-webserver`ì™€ `airflow-scheduler` ì„œë¹„ìŠ¤ ëª¨ë‘

#### 2ë‹¨ê³„: DAG ì„¤ì • ê°œì„ 

```python
# simple_kafka_etl.py ê°œì„ 
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 18),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,  # âœ… ê³¼ê±° ë°ì´í„° ì²˜ë¦¬ ì•ˆ í•¨
}

dag = DAG(
    'simple_kafka_etl',
    default_args=default_args,
    description='ê°„ë‹¨í•œ Kafka to PostgreSQL ETL',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,  # âœ… ë™ì‹œ ì‹¤í–‰ ì œí•œ
    tags=['kafka', 'postgresql', 'etl', 'simple']
)
```

#### 3ë‹¨ê³„: ìŠ¤í¬ë¦½íŠ¸ ê°œì„ 

```bash
# test-etl-pipeline.sh ê°œì„ 
# ì„±ê³µí•œ DAG ì‹¤í–‰ (ì•ˆì •ì ì¸ ë°©ì‹)
echo "simple_kafka_etl DAG ì‹¤í–‰ ì¤‘..."

# DAG í™œì„±í™” í™•ì¸
echo "DAG í™œì„±í™” í™•ì¸..."
docker exec airflow-webserver airflow dags unpause simple_kafka_etl

# ì ì‹œ ëŒ€ê¸°
sleep 5

# DAG íŠ¸ë¦¬ê±°
echo "DAG íŠ¸ë¦¬ê±° ì¤‘..."
docker exec airflow-scheduler airflow dags trigger simple_kafka_etl

# ì ì‹œ ëŒ€ê¸° í›„ ìƒíƒœ í™•ì¸
echo "DAG ì‹¤í–‰ ìƒíƒœ í™•ì¸..."
sleep 15

# DAG ì‹¤í–‰ ìƒíƒœ í™•ì¸
echo "DAG ì‹¤í–‰ ëª©ë¡:"
docker exec airflow-webserver airflow dags list-runs -d simple_kafka_etl --limit 3
```

### ì„±ê³µ ê²°ê³¼

#### ìµœì¢… ì„±ê³µ ë¡œê·¸

```bash
ğŸš€ 11ë‹¨ê³„: Airflow ETL DAG ì‹¤í–‰
ì„±ê³µí•œ simple_kafka_etl DAG ì‹¤í–‰ ì¤‘...
PostgreSQL ì—°ê²° í™•ì¸ ë° ìƒì„±...
Successfully added `conn_id`=postgres_dw : postgres://dwuser:******@postgres-dw:5432/bankdw
simple_kafka_etl DAG ì‹¤í–‰ ì¤‘...
DAG í™œì„±í™” í™•ì¸...
Dag: simple_kafka_etl, paused: False  # âœ… í™œì„±í™”ë¨
DAG íŠ¸ë¦¬ê±° ì¤‘...
DAG ì‹¤í–‰ ìƒíƒœ í™•ì¸...
```

#### PostgreSQL ê²°ê³¼

**í˜„ì¬ ìƒíƒœ í…Œì´ë¸”**: 3ê°œ ë ˆì½”ë“œ

```
 id | original_id | user_id |           account           | original_registered_at |      last_updated_at
----+-------------+---------+-----------------------------+------------------------+----------------------------
  1 |           1 |    1001 | KB Bank 999-888-777666      | 2025-09-18 12:43:29    | 2025-09-18 12:45:30.167309
  2 |           2 |    1002 | Shinhan Bank 987-654-321098 | 2025-09-18 03:43:29    | 2025-09-18 12:45:30.163069
  4 |           4 |    1004 | Hana Bank 111-222-333444    | 2025-09-18 12:45:01    | 2025-09-18 12:45:30.166177
```

**íˆìŠ¤í† ë¦¬ í…Œì´ë¸”**: 7ê°œ ë ˆì½”ë“œ (ëª¨ë“  ë³€ê²½ ì´ë²¤íŠ¸ ê¸°ë¡)

### í•µì‹¬ í•´ê²° í¬ì¸íŠ¸

#### 1. Airflow ì„¤ì • ìˆ˜ì •

- `AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"`ë¡œ ë³€ê²½
- DAGê°€ ìƒì„±ë  ë•Œ ìë™ìœ¼ë¡œ í™œì„±í™”ë˜ë„ë¡ ì„¤ì •

#### 2. DAG ì„¤ì • ê°œì„ 

- `catchup=False`: ê³¼ê±° ë°ì´í„° ì²˜ë¦¬ ë°©ì§€
- `max_active_runs=1`: ë™ì‹œ ì‹¤í–‰ ì œí•œìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

#### 3. ìŠ¤í¬ë¦½íŠ¸ ê°œì„ 

- DAG í™œì„±í™” í™•ì¸ ë‹¨ê³„ ì¶”ê°€
- ìƒíƒœ ì²´í¬ ë¡œì§ ê°•í™”
- ìë™ ì‹¤í–‰ì„ ìœ„í•œ ì•ˆì •ì ì¸ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•

### êµí›ˆ

1. **Airflow ì„¤ì •ì˜ ì¤‘ìš”ì„±**: ê¸°ë³¸ ì„¤ì •ì´ DAG ì‹¤í–‰ì— í° ì˜í–¥ì„ ë¯¸ì¹¨
2. **ë‹¨ê³„ë³„ í™•ì¸**: DAG í™œì„±í™” ìƒíƒœë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
3. **ì„¤ì • ë¬¸ì„œí™”**: ê° ì„¤ì •ì˜ ì˜ë¯¸ì™€ ì˜í–¥ì„ ëª…í™•íˆ ì´í•´í•  í•„ìš”
4. **ìë™í™” ê²€ì¦**: ìŠ¤í¬ë¦½íŠ¸ì˜ ìë™ ì‹¤í–‰ì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ ë°˜ë“œì‹œ ê²€ì¦

---

## ğŸš€ ìµœì¢… ì•„í‚¤í…ì²˜

```
MySQL OLTP DB â†’ Debezium â†’ Kafka â†’ Airflow ETL â†’ PostgreSQL OLAP DB
     âœ…              âœ…        âœ…         âœ…              âœ…
```

**ì‹¤ì‹œê°„ ë°ì´í„° ë³€ê²½ì‚¬í•­ì´ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥ë¨**

### ìµœì¢… ì„±ê³µ í™•ì¸

âœ… **CDC íŒŒì´í”„ë¼ì¸**: MySQL â†’ Debezium â†’ Kafka ì™„ë²½í•˜ê²Œ ì‘ë™
âœ… **ETL íŒŒì´í”„ë¼ì¸**: Kafka â†’ Airflow â†’ PostgreSQL ì™„ë²½í•˜ê²Œ ì‘ë™  
âœ… **ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”**: INSERT, UPDATE, DELETE ëª¨ë‘ ì²˜ë¦¬
âœ… **Airflow ìë™ ì‹¤í–‰**: ì„¤ì • ìˆ˜ì •ìœ¼ë¡œ DAGê°€ ìë™ìœ¼ë¡œ ì‹¤í–‰ë¨
âœ… **ìŠ¤í¬ë¦½íŠ¸ ìë™í™”**: `./test-etl-pipeline.sh` ì‹¤í–‰ ì‹œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
